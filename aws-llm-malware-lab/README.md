# AWS LLM Malware Testing Laboratory

Modular Terraform codebase that provisions an isolated AWS environment for detonating and analyzing potentially malicious LLM model weights. The architecture enforces a **disconnected VPC paradigm**, **zero-trust data access**, and **cost-optimized compute** via GPU Spot instances.

> **Warning:** This lab is designed for authorized security research only. Ensure you have proper authorization before deploying and detonating any models.

## Architecture Overview

```
                         Internet
                            |
                      +-----------+
                      |    IGW    |
                      +-----------+
                            |
              +----------------------------+
              |      Public Subnet         |
              |   (egress / ingestion)     |
              |                            |
              |  +----------------------+  |
              |  | NAT Instance         |  |
              |  | t4g.nano ARM         |  |
              |  | IP Masquerading      |  |
              |  +----------------------+  |
              +----------------------------+
                            |
              +----------------------------+
              |      Private Subnet        |
              |     (detonation tier)      |
              |                            |
              |  +----------------------+  |
              |  | GPU Spot Instances   |  |
              |  | g4dn.xlarge/g5.xlarge|  |
              |  | Ollama + NVIDIA      |  |
              |  +----------------------+  |
              |            |               |
              |   +------------------+     |
              |   | S3 VPC Endpoint  |---->| S3 Bucket (model weights)
              |   +------------------+     |  SSE-KMS, VPCE-only policy
              +----------------------------+

    Access: SSM Session Manager only (no SSH/RDP)
    DNS:    Route 53 Resolver Firewall (allowlist)
    Logs:   VPC Flow Logs + SSM Session Logs -> CloudWatch
    Scan:   GuardDuty Malware Protection + EventBridge
```

## Directory Structure

```
aws-llm-malware-lab/
├── backend.tf                        # S3 remote state + DynamoDB locking
├── providers.tf                      # AWS provider (no hardcoded creds)
├── variables.tf                      # Root-level input variables
├── environments/
│   └── dev/
│       ├── main.tf                   # Module composition & wiring
│       └── terraform.tfvars          # Dev environment values
└── modules/
    ├── network/                      # VPC, subnets, NAT instance, NACLs, DNS Firewall
    ├── security/                     # IAM roles, SSM, ABAC, permissions boundary, flow logs
    ├── storage/                      # S3 bucket, KMS CMK, VPCE policy, GuardDuty, EventBridge
    └── compute/                      # GPU Spot ASG, launch template, Ollama bootstrap
```

## Module Summary

### Network (`modules/network/`)

- Standalone VPC with no peering, Transit Gateway, Direct Connect, or VPN attachments
- Public subnet with IGW for egress; private subnet with no public IPs
- Self-managed **t4g.nano ARM NAT instance** with iptables masquerading (no managed NAT Gateway)
- Default security group replaced with deny-all; detonation SG allows only HTTPS/HTTP/DNS outbound
- NACLs with dynamic deny rules for known-malicious CIDRs and restricted ephemeral ports (32768-61000)
- Route 53 Resolver DNS Firewall: allowlist for AWS, Ollama, HuggingFace, PyPI; NXDOMAIN for all else

### Storage (`modules/storage/`)

- S3 bucket for model weight ingestion with versioning enabled
- Block Public Access enabled, legacy ACLs disabled (BucketOwnerEnforced)
- SSE-KMS encryption with a Customer Managed Key (key rotation enabled)
- Bucket policy denies `s3:GetObject` / `s3:PutObject` unless `aws:sourceVpce` matches the S3 gateway endpoint
- EventBridge rule fires on every `Object Created` event
- GuardDuty detector with S3 logs and malware protection data sources enabled

### Security (`modules/security/`)

- EC2 execution role with SSM core policy and scoped S3/KMS access
- **Permissions boundary** that explicitly denies IAM escalation actions
- ABAC policy restricting `ssm:StartSession` to instances tagged `Environment=MalwareLab`
- SSM Session Manager document logging all terminal commands to CloudWatch (90-day retention)
- VPC Flow Logs capturing all interface telemetry to CloudWatch (90-day retention)

### Compute (`modules/compute/`)

- Launch template with **100% Spot** allocation via `mixed_instances_policy`
- `capacity-optimized` Spot strategy across `g4dn.xlarge` and `g5.xlarge`
- IMDSv2 enforced, 100 GiB encrypted gp3 root volume
- UserData bootstrap: system update, NVIDIA driver install, Ollama install & systemd enablement
- Instances tagged `Environment=MalwareLab` for ABAC-gated SSM access

## Prerequisites

1. **AWS account** with appropriate permissions to create VPCs, EC2, S3, IAM, KMS, GuardDuty, and Route 53 resources.

2. **Terraform state infrastructure** — create these before `terraform init`:

   ```bash
   # S3 bucket for remote state
   aws s3api create-bucket \
     --bucket llm-malware-lab-tfstate \
     --region us-east-1

   aws s3api put-bucket-versioning \
     --bucket llm-malware-lab-tfstate \
     --versioning-configuration Status=Enabled

   # DynamoDB table for state locking
   aws dynamodb create-table \
     --table-name llm-malware-lab-tflock \
     --attribute-definitions AttributeName=LockID,AttributeType=S \
     --key-schema AttributeName=LockID,KeyType=HASH \
     --billing-mode PAY_PER_REQUEST \
     --region us-east-1
   ```

3. **AWS credentials** configured via environment variables, SSO, or instance profile — no static keys in code.

## Deployment

1. **Configure variables** — edit `environments/dev/terraform.tfvars`:

   ```hcl
   # Required: replace with real IAM ARNs
   analyst_principal_arns = ["arn:aws:iam::123456789012:user/analyst"]
   kms_admin_arns         = ["arn:aws:iam::123456789012:role/admin"]

   # Optional: known-malicious CIDRs to block at NACL level
   blocked_cidrs = ["203.0.113.0/24", "198.51.100.0/24"]
   ```

2. **Initialize and plan**:

   ```bash
   cd environments/dev
   terraform init
   terraform plan -var-file=terraform.tfvars
   ```

3. **Apply**:

   ```bash
   terraform apply -var-file=terraform.tfvars
   ```

4. **Connect to detonation hosts** (SSM only):

   ```bash
   aws ssm start-session --target <instance-id>
   ```

5. **Download and run a model**:

   ```bash
   # On the detonation host
   aws s3 cp s3://$MODEL_BUCKET/suspicious-model.gguf /tmp/model.gguf
   ollama serve &
   ollama create test-model -f /tmp/Modelfile
   ollama run test-model "test prompt"
   ```

## Teardown

```bash
cd environments/dev
terraform destroy -var-file=terraform.tfvars
```

## Key Design Decisions

| Decision | Rationale |
|---|---|
| Self-managed NAT instance over NAT Gateway | Saves ~$32/month; t4g.nano costs ~$3/month vs ~$35 for managed NAT GW |
| 100% Spot with capacity-optimized strategy | GPU Spot can save 60-70% over on-demand; capacity-optimized minimizes interruptions |
| SSM-only access (no SSH/RDP) | Zero inbound ports open; all sessions logged; no key management overhead |
| DNS Firewall allowlist | Prevents C2 callbacks and data exfiltration to unauthorized domains |
| S3 VPCE-only bucket policy | Model weights can only be accessed from within the isolated VPC |
| Permissions boundary on EC2 role | Prevents a compromised instance from escalating IAM privileges |
| IMDSv2 enforced | Mitigates SSRF-based credential theft from the instance metadata service |
| GuardDuty + EventBridge | Automated scanning of uploaded models; alerting pipeline for detections |
| Bedrock for evaluation (not provisioned) | Evaluator models run serverless via API — no additional infrastructure needed |

## Security Considerations

- The lab VPC has **no connectivity** to any corporate or production network
- All outbound DNS is filtered; only explicitly allowlisted domains resolve
- NACLs provide a secondary layer of defense at the subnet boundary
- VPC Flow Logs and SSM session logs provide full forensic auditability
- The S3 bucket is only accessible via the VPC endpoint — not from the public internet or other VPCs
- The KMS CMK key policy is scoped; the root account retains admin access for recovery
